{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jigsaw TPU:XLM-Roberta\n",
    "- 주요 초점 영역은  `독성을 식별할 수 있는 머신러닝 모델`\n",
    "- 독성은 무례하거나 무례한 것으로 정의되거나 \n",
    "- 누군가 토론을 떠날 가능성이 있는 것으로 정의된다.\n",
    "- 만약 이러한 독성 기여가 확인된다면, 우리는 더 안전하고, 더 협력적인 인터넷을 가질 수 있을 것이다.\n",
    "- 올해는 새로운 TPU 지원을 활용, \n",
    "- `영어 전용 훈련 데이터`로 `다국어 모델 구축`에 도전한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "#from kaggle_datasets import KaggleDatasets\n",
    "import transformers\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n",
    "    #chunk_size = 256 .. 큰 덩어리 사이즈\n",
    "    # tokenizer의 라이브러리 사용\n",
    "    # 각각 문장의 시작 부분이나 끝 부분 또는 단어 시퀀스 중 \n",
    "    # maxlen(=512)보다 큰 시퀀스에서 값을 제거한다.\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    # padding: 'pre' or 'post', pad either before or after each sequence.\n",
    "    # max_sentences: The max sentence length to use.\n",
    "    # 문장 길이는 최대 512로\n",
    "    tokenizer.enable_padding(max_length=maxlen)\n",
    "\n",
    "    all_ids = []\n",
    "    # texts의 처음부터 끝까지 chunk_size(=256)씩 건너띄면서\n",
    "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
    "        # text_chunk는 해당 인덱스에서 256개문자의 data\n",
    "        # tolist() = data를 list형태로\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        # tokenize multiple sentences at once;\n",
    "        # 한번에 여러문장들을 토큰화\n",
    "        #....백만건에 문장을 10초로\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        # encs의 문자들이 all_ids에 덮어진다\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "        \n",
    "    # 배열 생성함수\n",
    "    # 순서가 있는 객체(주로 리스트)를 넘겨받아 데이터가 들어있는 새로운 NumPy 배열 생성\n",
    "    return np.array(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코드 작업은 단어를 벡터(Vector)로 변환해 단어의 의미를 캡슐화하는 것으로, \n",
    "# 비슷한 단어의 숫자가 더 가깝다.\n",
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    # encode the word to vector of integer\n",
    "    # 단어를 정수의 벡터로 인코딩\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        # masking 처리가 된 토큰들에게 attention점수를 준 값 반환 X\n",
    "        return_attention_masks=False, \n",
    "        # type_ids = 형식 식별자 목록\n",
    "        # string_id 색인 기준으로 정렬해야하며 중복항복이 포함되지 않는 목록\n",
    "        return_token_type_ids=False,\n",
    "        # First sentence will have some PADDED tokens to match second sequence length\n",
    "        # 첫 번째 문장은 두 번째 시퀀스 길이와 일치하는 PADED 토큰이 (TRUE) 있을 것이다.\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen   // 512\n",
    "    )\n",
    "    # 배열 생성함수\n",
    "    # 순서가 있는 객체(주로 리스트)를 넘겨받아 데이터가 들어있는 새로운 NumPy 배열 생성\n",
    "    return np.array(enc_di['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert 언어모델\n",
    "- Bi-directional Encoder Representations from Transformers\n",
    "\n",
    "`간단한 설명`\n",
    "![image.png](https://lh3.googleusercontent.com/proxy/Qa_FsiWCU2pwdpnKKAhjJAaUZdB9P0HOm3BUXVx0_Ycomo-YYBEyWRe5LP6bSfpj2X9QSinZM9GeEJntjNAy4E4Pg66gCu1MottnGbqbsNZOuEwjbJAUhR81p3JrhuIhHwf88OMq00cmWYWPXHKOOg)\n",
    "\n",
    "- input은 sectence 문장 두개로 입력\n",
    "- sentence는 token 단위로 imbedded\n",
    "- imbedded된 것은 transformer layer 12개를 거친 후에 본인을 표현\n",
    "\n",
    "![image.png](https://mino-park7.github.io/images/2019/02/bert-input-representation.png)\n",
    "- cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(transformer, max_len=512):\n",
    "    #Input: for define input layer \n",
    "    #shape is vector with 512-dimensional vectors\n",
    "    #512차원 벡터를 가진 텐서 반환\n",
    "    # dtype(data type) : tf.int32 (int32 정수)\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\") # 입력 layer 정의\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "     \n",
    "    # to get the vector\n",
    "    # cls\n",
    "    # 모든 sentence의 첫번째 token은 언제나 cls를 갖는다\n",
    "    # 문장의 시작을 알리는 토큰\n",
    "    # transformer의 encoding을 마치면 cls토큰은 \n",
    "    # 나머지 이후 토큰들의 의미들을 응축하게 된다\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    \n",
    "    # define output layer\n",
    "    # Dense layer : 모든 입력 뉴런과 출력 뉴런을 연결하는 전 결합층\n",
    "    # activation = 'sigmond'함수는 활성화 함수로\n",
    "    # 입력되는 값을 0 과 1 사이의 값으로 출력 됩니다.\n",
    "   \n",
    "    out = Dense(1, activation='sigmoid')(cls_token)\n",
    "    # cls token과 레이어가 매끄럽게 학습될 수 있도록 한다\n",
    "    # initiate the model with inputs and outputs\n",
    "    \n",
    "    model = Model(inputs=input_word_ids, outputs=out)\n",
    "    # model.compile()에서 Adam optimizer 사용\n",
    "    # lr: 0보다 크거나 같은 float 값. 학습률.\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPU Configs\n",
    "\n",
    "## Run Bert Model on TPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**데이터의 tokenizing**\n",
    "\n",
    "ex) He likes playing -> He likes play ##ing\n",
    "\n",
    "- 입력문장을 tokenizing하고, 빈도수에 따라 분리\n",
    "- input 문장 두개의 token sequence 가 학습에 사용\n",
    "- 위의 그림 참고\n",
    "![image.png](https://mino-park7.github.io/images/2019/02/bert-input-representation.png)\n",
    "\n",
    "### 전처리 이후 tokenize 필요\n",
    "- 빈도수에 기반해 단어를 의미있는 패턴으로 잘라서 tokenizing = word piece tokenizing\n",
    "\n",
    "*1) 빈도수를 검색해서 vocab후보를 계속해서 update -> 최종*\n",
    "\n",
    "*2) BERT 뒷단어에 ##을 붙여서 구별 (같은 글자라고 맨앞과 앞이 아닌거는 다르므로)*\n",
    "\n",
    "*3) vocab 후보를 기준으로 Bigrampairs를 만들고 빈도수를 계산해 가장 많이 등장한 Best pair를 하나로 합친다*\n",
    "\n",
    "*4) 이 과정을 반복해 최종 vocab에 리스트 저장-> 정해진 iteration 모두 수행*\n",
    "\n",
    "### INPUT\n",
    "앞에는 [cls] dog is pretty 뒤에는 [sep] 토큰\n",
    "\n",
    "*1) 입력token에 대한 word imbedding*\n",
    "\n",
    "*2) segment imbedding은 첫번째 문장인지, 두번째 문장인지 구별*\n",
    "\n",
    "*3) position imbedding 빈도의 개수에 따라 position imbedding을 다르게 해서 학습*\n",
    "\n",
    "*BERTbase = 4개의 TPU사용*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "# TPU :Tensor Processing Unit\n",
    "# 구글에서 2016년 5월에 발표한 데이터 분석 및 딥러닝용 하드웨어이다.\n",
    "# CPU나 GPU에서 학습시킬 때와는 비교도 안되는 수준의\n",
    "# 학습 속도를 보입니다\n",
    "# TPU의 사용방법은 크게 두가지\n",
    "# 1) TPUstimator 와 2) TPUstrategy\n",
    "# 학습 중 상태 값들과 계산 워크로드의 분산을 위해 설정하는 정책\n",
    "# 객체를 활용해서 데이터나 연산 부담을 여러기기에 분산\n",
    "\n",
    "try:  # 실행할 코드\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # TPUClusterResolver : 하나 혹은 여러 대의 TPU 집합을 잡는다\n",
    "    # tpu : 문자열 또는 사용할 TPU에 해당하는 문자열 목록. \n",
    "    # 단일 문자열이 빈 문자열, 'local' 문자열 또는 'grpc://' 또는 '/bns'로 시작하는 문자열인 경우, \n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master()) \n",
    "    # grpc경로 반환 : grpc://10.0.0.2:8470\n",
    "except ValueError: # 예외가 발생했을 때 처리하는 코드\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    # experimental_connect_to_cluster 함수와 \n",
    "    # initialize_tpu_system 함수를 통해 TPU와 런타임을 연결한다\n",
    "    \n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    # 그 후에는 strategy를 생성\n",
    "    # TPU의 집합을 데이터 분산 대상에 넣어준다\n",
    "    # tf.distribute.experimental.TPUStrategy\n",
    "    # 텐서 플로 훈련을 텐서처리장치에서 수행하는 전략\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "# 복제본의 수\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTO = tf.data.experimental.AUTOTUNE\n",
    "# 작동하는 Network가 스스로 설정하고 \n",
    "# Dataset을 잘 불러올 수 있게 결정\n",
    "# 많은 양의 Data를 처리하는 경우 Load를 자동으로 해주기 때문에\n",
    "# 속도가 빨라질 것으로 예상\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Data access\n",
    "#GCS_DS_PATH = KaggleDatasets().get_gcs_path()\n",
    "\n",
    "# Configuration\n",
    "# EPOCHS 는 전체 데이터 셋에 대해 forward pass/backward pass 과정을 거친 것\n",
    "# 전체 데이터 셋에 대해 한번 학습을 완료한 상태\n",
    "EPOCHS = 2 \n",
    "# 전체 데이터를 2번 사용해서 학습을 거치는 것\n",
    "# BATCH_SIZE : 한 번의 batch마다 주는 데이터 샘플의 size\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync # 복제본 8\n",
    "MAX_LEN = 192 \n",
    "# XLM-Roberta 모델 이용 ( bert보다 성능 우수)\n",
    "# 변압기 아키텍처 훈련과 아키텍처의 확장 우수\n",
    "MODEL = 'jplu/tf-xlm-roberta-large'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://mblogthumb-phinf.pstatic.net/MjAxOTAxMjNfMjU4/MDAxNTQ4MjM1Nzg3NTA2.UtvnGsckZhLHOPPOBWH841IWsZFzNcgwZvYKi2nxImEg.CdtqIxOjWeBo4eNBD2pXu5uwYGa3ZVUr8WZvtldArtYg.PNG.qbxlvnf11/20190123_182720.png?type=w800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the real tokenizer 실제의 tokenizer 로드\n",
    "# 토큰화(문자열 분할), 토큰 문자열 인코딩과 디코딩(정수로 변환)\n",
    "# 새로운 토큰들 추가, 마스크 관리 등 이루어진다\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각데이터에서 중요한 것은 comment_text 칼럼\n",
    "# 독성이 있는 것 or 독성이 없는 것으로 분류된 텍스트\n",
    "# 독성이 있는 논평은 1점을 받고, 독성이 없는 논평은 0점\n",
    "\n",
    "train1 = pd.read_csv(\"./jigsaw-toxic-comment-train.csv\")\n",
    "train2 = pd.read_csv(\"./jigsaw-unintended-bias-train.csv\")\n",
    "# train2의 toxic값을 반올림->int 형변환\n",
    "train2.toxic = train2.toxic.round().astype(int)\n",
    "\n",
    "valid = pd.read_csv('./validation.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "sub = pd.read_csv('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>536378</th>\n",
       "      <td>\"Natural justice\" and \"natural rights\" are ver...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486306</th>\n",
       "      <td>\"Like a bad food critic, the MSM concentrated ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180553</th>\n",
       "      <td>he didn't mean it, he's sorry so back off please</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32462</th>\n",
       "      <td>The legislators have a lot on their plate to d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921647</th>\n",
       "      <td>Let's assume that there is only so much money ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  toxic\n",
       "536378  \"Natural justice\" and \"natural rights\" are ver...      0\n",
       "486306  \"Like a bad food critic, the MSM concentrated ...      0\n",
       "180553   he didn't mean it, he's sorry so back off please      0\n",
       "32462   The legislators have a lot on their plate to d...      0\n",
       "921647  Let's assume that there is only so much money ...      0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine train1 with a subset of train2\n",
    "train = pd.concat([\n",
    "    train1[['comment_text', 'toxic']],\n",
    "    # query : 조건식 (문자열)\n",
    "    train2[['comment_text', 'toxic']].query('toxic==1'),\n",
    "    # n = 추출할 샘플의 수\n",
    "    # random_state : 랜덤 샘플 추출 시 시드 입력\n",
    "    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=100000, random_state=0)\n",
    "])\n",
    "\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-bed2180288db>\u001b[0m in \u001b[0;36mregular_encode\u001b[1;34m(texts, tokenizer, maxlen)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# 첫 번째 문장은 두 번째 시퀀스 길이와 일치하는 PADED 토큰이 (TRUE) 있을 것이다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mpad_to_max_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxlen\u001b[0m   \u001b[1;33m//\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     )\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# 배열 생성함수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, max_length, stride, truncation_strategy, pad_to_max_length, is_pretokenized, return_tensors, return_token_type_ids, return_attention_masks, return_overflowing_tokens, return_special_tokens_masks, return_offsets_mapping, return_lengths, **kwargs)\u001b[0m\n\u001b[0;32m   1767\u001b[0m                 \u001b[0mreturn_special_tokens_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_special_tokens_masks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1768\u001b[0m                 \u001b[0mreturn_lengths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_lengths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1769\u001b[1;33m                 \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# We will convert the whole batch to tensors at the end\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1770\u001b[0m             )\n\u001b[0;32m   1771\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36mprepare_for_model\u001b[1;34m(self, ids, pair_ids, max_length, add_special_tokens, stride, truncation_strategy, pad_to_max_length, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_lengths)\u001b[0m\n\u001b[0;32m   1927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1928\u001b[0m         \u001b[1;31m# Check lengths\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1929\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1930\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_max_length\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1931\u001b[0m             logger.warning(\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# regul_encode 함수 : 단어를 정수의 벡터(Vector)로 변환해 단어의 의미를 캡슐화\n",
    "x_train = regular_encode(train.comment_text.values, tokenizer, maxlen = MAX_LEN)\n",
    "x_valid = regular_encode(valid.comment_text.values, tokenizer, maxlen = MAX_LEN)\n",
    "x_test = regular_encode(test.content.values, tokenizer, maxlen = MAX_LEN)\n",
    "\n",
    "y_train = train.toxic.values\n",
    "y_valid = valid.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-6446f3daa999>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m train_dataset = (\n\u001b[0;32m      2\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2048\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "# TPU는 하나의 학습 단계를 실행하는데 필요한 시간을 급격하게 감소\n",
    "# 최대 성능 위해 현재 단계가 종료되기 전에 다음 스텝의 데이터를 운반하는 효율적인 입력 파이프라인이 필요\n",
    "# tf.data API는 유연하고 효율적인 입력 파이프라인을 만드는데 도움이 됩니다\n",
    "\n",
    "# Dataset 사용하려면\n",
    "# 1) 데이터 불러오기\n",
    "# 2) Iterator(반복자) 생성하기\n",
    "# 3) 데이터 사용하기\n",
    "train_dataset = (\n",
    "    # x_train과 y_train으로 dataset 생성\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_train, y_train))\n",
    "    # repeat(step_n) : 원하는 epoch 수를 넣을 수 있음\n",
    "    .repeat() # 파라미터 X : iteration이 무제한으로 돌아감\n",
    "    .shuffle(2048) # 한번 epoch가 돌고나서 랜덤하게 섞을 것 정함\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO) # 연산에 필요한 data들을 미리 가져옴\n",
    ")\n",
    "\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_valid, y_valid))\n",
    "    .batch(BATCH_SIZE)\n",
    "    # .cashe() : preprocessing 시간이 너무 길어서 줄이고 싶을때 사용\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(x_test)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a00680e8b1b4c4dae89c8f59027cf82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=3271420488.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_word_ids (InputLayer)  [(None, 192)]             0         \n",
      "_________________________________________________________________\n",
      "tf_roberta_model (TFRobertaM ((None, 192, 1024), (None 559890432 \n",
      "_________________________________________________________________\n",
      "tf_op_layer_strided_slice (T [(None, 1024)]            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 559,891,457\n",
      "Trainable params: 559,891,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Wall time: 6min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 이전에 distribute Strategy\n",
    "# 모델을 불어올 때 범위(scope)를 지정해도 되고 안해도 됨 \n",
    "with strategy.scope():\n",
    "    # 사전 교육된 MODEL에서 TF AutoModel 구현\n",
    "    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
    "    # build model 함수 호출\n",
    "    model = build_model(transformer_layer, max_len=MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = x_train.shape[0] // BATCH_SIZE\n",
    "# 모델 학습\n",
    "train_history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=n_steps, # 한 epoch에 사용한 스텝 수\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=EPOCHS  # 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = x_valid.shape[0] // BATCH_SIZE\n",
    "train_history_2 = model.fit(\n",
    "    # .repeat() 원하는 epoch 수를 넣을 수 있음\n",
    "    valid_dataset.repeat(),\n",
    "    steps_per_epoch=n_steps, # 한 epoch에 사용한 스텝 수\n",
    "    epochs=EPOCHS # 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict : 트레이닝이 끝난 모델을 분류수행\n",
    "sub['toxic'] = model.predict(test_dataset, verbose=1)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
