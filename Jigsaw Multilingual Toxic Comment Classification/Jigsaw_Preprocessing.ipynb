{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jigsaw TPU:XLM-Roberta\n",
    "- 주요 초점 영역은  `독성을 식별할 수 있는 머신러닝 모델`\n",
    "- 독성은 무례하거나 무례한 것으로 정의되거나 \n",
    "- 누군가 토론을 떠날 가능성이 있는 것으로 정의된다.\n",
    "- 만약 이러한 독성 기여가 확인된다면, 우리는 더 안전하고, 더 협력적인 인터넷을 가질 수 있을 것이다.\n",
    "- 올해는 새로운 TPU 지원을 활용, \n",
    "- `영어 전용 훈련 데이터`로 `다국어 모델 구축`에 도전한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import transformers\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n",
    "    #chunk_size = 256 .. 큰 덩어리 사이즈\n",
    "    # tokenizer의 라이브러리 사용\n",
    "    # 각각 문장의 시작 부분이나 끝 부분 또는 단어 시퀀스 중 \n",
    "    # maxlen(=512)보다 큰 시퀀스에서 값을 제거한다.\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    # padding: 'pre' or 'post', pad either before or after each sequence.\n",
    "    # max_sentences: The max sentence length to use.\n",
    "    # 문장 길이는 최대 512로\n",
    "    tokenizer.enable_padding(max_length=maxlen)\n",
    "\n",
    "    all_ids = []\n",
    "    # texts의 처음부터 끝까지 chunk_size(=256)씩 건너띄면서\n",
    "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
    "        # text_chunk는 해당 인덱스에서 256개문자의 data\n",
    "        # tolist() = data를 list형태로\n",
    "        text_chunk = texts[i:i+chunk_size].tolist()\n",
    "        # tokenize multiple sentences at once;\n",
    "        # 한번에 여러문장들을 토큰화\n",
    "        #....백만건에 문장을 10초로\n",
    "        encs = tokenizer.encode_batch(text_chunk)\n",
    "        # encs의 문자들이 all_ids에 덮어진다\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "        \n",
    "    # 배열 생성함수\n",
    "    # 순서가 있는 객체(주로 리스트)를 넘겨받아 데이터가 들어있는 새로운 NumPy 배열 생성\n",
    "    return np.array(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코드 작업은 단어를 벡터(Vector)로 변환해 단어의 의미를 캡슐화하는 것으로, \n",
    "# 비슷한 단어의 숫자가 더 가깝다.\n",
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    # encode the word to vector of integer\n",
    "    # 단어를 정수의 벡터로 인코딩\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        # masking 처리가 된 토큰들에게 attention점수를 준 값 반환 X\n",
    "        return_attention_masks=False, \n",
    "        # type_ids = 형식 식별자 목록\n",
    "        # string_id 색인 기준으로 정렬해야하며 중복항복이 포함되지 않는 목록\n",
    "        return_token_type_ids=False,\n",
    "        # First sentence will have some PADDED tokens to match second sequence length\n",
    "        # 첫 번째 문장은 두 번째 시퀀스 길이와 일치하는 PADED 토큰이 (TRUE) 있을 것이다.\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen   // 512\n",
    "    )\n",
    "    # 배열 생성함수\n",
    "    # 순서가 있는 객체(주로 리스트)를 넘겨받아 데이터가 들어있는 새로운 NumPy 배열 생성\n",
    "    return np.array(enc_di['input_ids'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert 언어모델\n",
    "- Bi-directional Encoder Representations from Transformers\n",
    "\n",
    "`간단한 설명`\n",
    "![image.png](https://lh3.googleusercontent.com/proxy/GArT-81Yb10AFcmExPn2ODye7r8H0nnUl7v1FlbIBEGkgW5ndBkmBWrccavdhHFLl2AXekJK3WKUSF4CtNhsdP8nX7yowQJewpoW63j7__02YFMZvXSIgAwWlaAeEDEJb1Gc4QAU-0Be4EKhnUjvVA)\n",
    "\n",
    "- input은 sectence 문장 두개로 입력\n",
    "- sentence는 token 단위로 imbedded\n",
    "- imbedded된 것은 transformer layer 12개를 거친 후에 본인을 표현\n",
    "\n",
    "![image.png](https://mino-park7.github.io/images/2019/02/bert-input-representation.png)\n",
    "- cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(transformer, max_len=512):\n",
    "    #Input: for define input layer \n",
    "    #shape is vector with 512-dimensional vectors\n",
    "    #512차원 벡터를 가진 텐서 반환\n",
    "    # dtype(data type) : tf.int32 (int32 정수)\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\") # 입력 layer 정의\n",
    "    sequence_output = transformer(input_word_ids)[0]\n",
    "     \n",
    "    # to get the vector\n",
    "    # cls\n",
    "    # 모든 sentence의 첫번째 token은 언제나 cls를 갖는다\n",
    "    # 문장의 시작을 알리는 토큰\n",
    "    # transformer의 encoding을 마치면 cls토큰은 \n",
    "    # 나머지 이후 토큰들의 의미들을 응축하게 된다\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    \n",
    "    # define output layer\n",
    "    # Dense layer : 모든 입력 뉴런과 출력 뉴런을 연결하는 전 결합층\n",
    "    # activation = 'sigmond'함수는 활성화 함수로\n",
    "    # 입력되는 값을 0 과 1 사이의 값으로 출력 됩니다.\n",
    "   \n",
    "    out = Dense(1, activation='sigmoid')(cls_token)\n",
    "    # cls token과 레이어가 매끄럽게 학습될 수 있도록 한다\n",
    "    # initiate the model with inputs and outputs\n",
    "    \n",
    "    model = Model(inputs=input_word_ids, outputs=out)\n",
    "    # model.compile()에서 Adam optimizer 사용\n",
    "    # lr: 0보다 크거나 같은 float 값. 학습률.\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPU Configs\n",
    "\n",
    "## Run Bert Model on TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Data access\n",
    "GCS_DS_PATH = KaggleDatasets().get_gcs_path()\n",
    "\n",
    "# Configuration\n",
    "EPOCHS = 2\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "MAX_LEN = 192\n",
    "MODEL = 'jplu/tf-xlm-roberta-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the real tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\n",
    "train2 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\")\n",
    "train2.toxic = train2.toxic.round().astype(int)\n",
    "\n",
    "valid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\n",
    "test = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv')\n",
    "sub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train1 with a subset of train2\n",
    "train = pd.concat([\n",
    "    train1[['comment_text', 'toxic']],\n",
    "    train2[['comment_text', 'toxic']].query('toxic==1'),\n",
    "    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=100000, random_state=0)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "x_train = regular_encode(train.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "x_valid = regular_encode(valid.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
    "x_test = regular_encode(test.content.values, tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "y_train = train.toxic.values\n",
    "y_valid = valid.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_train, y_train))\n",
    "    .repeat()\n",
    "    .shuffle(2048)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((x_valid, y_valid))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(x_test)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
    "    model = build_model(transformer_layer, max_len=MAX_LEN)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = x_train.shape[0] // BATCH_SIZE\n",
    "train_history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=n_steps,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = x_valid.shape[0] // BATCH_SIZE\n",
    "train_history_2 = model.fit(\n",
    "    valid_dataset.repeat(),\n",
    "    steps_per_epoch=n_steps,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['toxic'] = model.predict(test_dataset, verbose=1)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
